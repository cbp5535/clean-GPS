{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Setup: The purpose of this code is to clean GPS device point data in recreation and tourism settings. In order to successfully run the code you will need:\n",
    "\n",
    "(i) GPS device data\n",
    "(ii) a shapefile of your study location\n",
    "\n",
    "If necessary, you may also need:\n",
    "\n",
    "(i) a shapefile of a buffer area for GPS device dropbox areas\n",
    "(ii) a shapefile of a buffer area for GPS device data download areas\n",
    "\n",
    "The setup assumes that all GPS tracks and files are saved and stored in the same folder with the appropriate projection system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcpy version: 3.1.2\n",
      "pandas version: 1.4.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "\n",
    "# Get and print the version of arcpy (version used 3.1.2)\n",
    "print(f\"arcpy version: {arcpy.GetInstallInfo()['Version']}\")\n",
    "\n",
    "# Get and print the version of pandas (version used 1.4.4)\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "###Set up key paths\n",
    "\n",
    "##Base directory folder to retrieve files from and save completed files to for the project\n",
    "base_dir = \"Your directory\"\n",
    "\n",
    "##Folder name for the folder containing the GPS data (must be within base directory)\n",
    "folder_with_data = \"Folder w/ data\"\n",
    "\n",
    "##The geodatabase you wish to use to temporarily store and work with files as you clean the data\n",
    "target_gdb = \"ArcGIS project geodatabase\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "##Identify location of relevant shapefiles\n",
    "\n",
    "##Shapefile of the destination boundary for your study site. Procedure for doing this involved creating a polygon around National Park Service-provided roads and trails in the South Rim and removing entrances/Forest service roads outside the park caught by polygon creation by hand.\n",
    "destination_boundary = os.path.join(base_dir, \"Name of destination boundary\")\n",
    "\n",
    "#For some studies, drop boxes may be located within the study boundary. If necessary, create an appropriately sized polygon for removing those points so that they do not generate noise, either by hand or using the buffer tool from the point. In this case, we used the buffer tool set at 150m.\n",
    "dropbox_buffer = os.path.join(base_dir, \"Name of dropbox buffer boundary\")\n",
    "\n",
    "#For some studies, data in GPS devices may be uploaded within the study site and devices may record \"ghost\" points when they are uploaded at researcher facilities. If necessary, create an appropriately sized polygon for removing points from those locations so that they do not generate noise, either by hand or using the buffer tool from the point. In this case, we used the buffer tool set at 75m from the worker cabins we downloaded points from.\n",
    "download_buffer = os.path.join(base_dir, \"Name of download buffer boundary\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "##Set your arcpy environment and permit overwrite\n",
    "\n",
    "arcpy.env.workspace = target_gdb\n",
    "arcpy.env.overwriteOutput = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 1: In this section you will do the high level importing and cleaning of your data. This will include importing all of your data into your geodatabase, pre-processing your fields, and removing points based on spatial parameters (i.e., destination boundary, dropbox boundary, download area boundary) relevant to your study site, as applicable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Import data. In order for this to work, all data must be recorded as a shape file.\n",
    "in_data = os.path.join(base_dir, folder_with_data)\n",
    "filter = \"*.shp\"\n",
    "\n",
    "arcpy.intelligence.BatchImportData(in_data, target_gdb, filter)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Combine all separate shapefiles for each unique device/participant into one file\n",
    "\n",
    "##Set the workspace\n",
    "workspace = target_gdb\n",
    "\n",
    "#Making an empty list to put filepaths to feature classes in\n",
    "feature_classes = []\n",
    "\n",
    "#Walking workspace recursively checking type, and appending filepath to list\n",
    "for dirpath, dirnames, filenames in arcpy.da.Walk(workspace,datatype=\"FeatureClass\", type=\"Point\"):\n",
    "    for filename in filenames:\n",
    "        desc = arcpy.Describe(os.path.join(dirpath, filename))\n",
    "        if desc.shapeType == \"Point\":\n",
    "            feature_classes.append(os.path.join(dirpath, filename))\n",
    "\n",
    "#Filename for saving\n",
    "output = \"raw_points\"\n",
    "\n",
    "#merging my list of feature classes to a new dataset\n",
    "arcpy.Merge_management(feature_classes, output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##It is unnecessary to retain most fields recorded by GPS devices, however, specific needs for each project may vary. Below are the minimum necessary fields to run the code.\n",
    "\n",
    "process_raw = os.path.join(target_gdb, \"raw_points\")\n",
    "\n",
    "raw_points = pd.DataFrame.spatial.from_featureclass(process_raw)\n",
    "\n",
    "##Select the field for the object identification (OID) created by ArcGIS\n",
    "OID = \"OBJECTID\"\n",
    "\n",
    "##Select the field for group identification (GID), which refers the participant's unique ID\n",
    "GID = \"tident\"\n",
    "\n",
    "##Select the field(s) for time\n",
    "time= \"ltime\"\n",
    "\n",
    "##Select the field from\n",
    "shape = \"SHAPE\"\n",
    "\n",
    "##Retained fields\n",
    "retained_fields = [OID,GID,time,shape]\n",
    "\n",
    "##Create dataframe of necessary points only\n",
    "raw_points = raw_points[retained_fields]\n",
    "\n",
    "##Export datapoints back to geodatabase\n",
    "raw_points.spatial.to_featureclass(os.path.join(target_gdb, \"processed_points\"))\n",
    "\n",
    "##Print pandas dataframe\n",
    "raw_points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use Convert Time Field to convert “ltime” to Date format (labeling may vary). Input time format may vary by device.\n",
    "\n",
    "in_table = \"processed_points\"\n",
    "input_time_field = time\n",
    "input_time_format = \"yyyy/MM/dd HH:mm:ss\"\n",
    "output_time_field = \"ltime_converted\"\n",
    "\n",
    "arcpy.management.ConvertTimeField(in_table, input_time_field, input_time_format, output_time_field)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clip new merged shapefile using a polygon of the study area (with the survey locations cut out of the polygon – use continue feature tool)\n",
    "\n",
    "in_features = \"processed_points\"\n",
    "clip_features = destination_boundary\n",
    "out_feature_class = \"processed_points_clip\"\n",
    "\n",
    "arcpy.analysis.Clip(in_features, clip_features, out_feature_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Remove points around dropbox using 150m buffer\n",
    "\n",
    "in_features = \"processed_points_clip\"\n",
    "erase_features = dropbox_buffer\n",
    "out_feature_class = \"processed_points_clip_dropbox\"\n",
    "\n",
    "arcpy.analysis.PairwiseErase(in_features, erase_features, out_feature_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Remove points around cabin using 75m buffer\n",
    "\n",
    "in_features = \"processed_points_clip_dropbox\"\n",
    "erase_features = download_buffer\n",
    "out_feature_class = \"processed_points_clip_dropbox_cabin\"\n",
    "\n",
    "arcpy.analysis.PairwiseErase(in_features, erase_features, out_feature_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 2: The preceding section provides the highest level of data cleaning, however, the subsequent sections allow the researcher to dig deeper into errors that may occur from working with GPS devices. Section 2 relies on automatic commands to systematically remove points from likely errors (e.g., devices being turned off, noise). It will require defining assumptions that are pertinent to the set up of your GPS devices and reasonable behaviors of your participants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the Points to Track Segments tool to create a new set of features where all points are converted to track segments. Use the participant identification number (in this case, “tident”) as the group field. Deselect Error On Duplicate Timestamps.\n",
    "\n",
    "in_features = \"processed_points_clip_dropbox_cabin\"\n",
    "date_field = \"ltime_converted\"\n",
    "out_feature_class = \"processed_lines\"\n",
    "group_field = GID\n",
    "include_velocity = \"INCLUDE_VELOCITY\"\n",
    "error_on_duplicate_timestamps = \"ALLOW_DUPLICATE_TIMESTAMPS\"\n",
    "\n",
    "arcpy.intelligence.PointsToTrackSegments(in_features, date_field, out_feature_class, group_field, error_on_duplicate_timestamps = \"ALLOW_DUPLICATE_TIMESTAMPS\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Assumptions:\n",
    "\n",
    "##The GPS devices used in this study relied on 15 second intervals and the fastest method for travel were by car, therefore, we assumed that distances between points above 465 meters (i.e., 70 mph) were likely due to error. Change the field preceding the \">\" sign to match the field indicating distance between points.\n",
    "\n",
    "speed_assumption = \"Your speed clause here\" ##Example: \"distance_m > 465\"\n",
    "\n",
    "##The GPS devices used in this study relied on 15 second intervals, therefore, we assumed that recordings with intervals above 20 seconds were indicators of error. Change the field preceding the \">\" sign to match the field indicating time between points.\n",
    "\n",
    "time_assumption = \"Your time clause here\" ##Exmample: \"dt_sec > 20\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select all attributes (line segments) with a distance greater than 465 meters (traveling greater than 70 miles per hour). Delete the selection.\n",
    "\n",
    "in_layer_or_view = \"processed_lines\"\n",
    "selection_type = \"NEW_SELECTION\"\n",
    "where_clause = speed_assumption\n",
    "\n",
    "Distance_Selection = arcpy.management.SelectLayerByAttribute(in_layer_or_view, selection_type, where_clause)\n",
    "\n",
    "\n",
    "##Delete selected attributes.\n",
    "arcpy.management.DeleteFeatures(Distance_Selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create new text fields equal to your group identification plus the unique point identification for each entry for further examination. First make sure your fields, such as identification, are text. These expressions should occur by default in your data by now, but revise them (i.e., the group ID, object ID) if they do not.\n",
    "\n",
    "in_table = \"processed_lines\"\n",
    "field = \"Text_OID\"\n",
    "expression = \"!OBJECTID!\"\n",
    "\n",
    "arcpy.management.CalculateField(in_table, field, expression, field_type = \"TEXT\")\n",
    "\n",
    "##Now make a field that combines Group Identification+OBJECTID.\n",
    "\n",
    "in_table = \"processed_lines\"\n",
    "field = \"ID_2\"\n",
    "expression = \"!group_id! + !Text_OID!\"\n",
    "\n",
    "arcpy.management.CalculateField(in_table, field, expression)\n",
    "\n",
    "###Remove Unnecessary Fields (again)\n",
    "\n",
    "in_table = \"processed_lines\"\n",
    "fields = [\"Text_OID\"]\n",
    "\n",
    "arcpy.DeleteField_management(in_table, fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the Feature Vertices to Points tool to transform the line segments of participant feature classes to points. Again, stay working in the same geodatabase. Use start vertex as the Point Type. Add the new point-based feature classes to the map.\n",
    "\n",
    "in_features = \"processed_lines\"\n",
    "out_feature_class = \"processed_points\"\n",
    "point_location = \"START\"\n",
    "\n",
    "arcpy.management.FeatureVerticesToPoints(in_features, out_feature_class, point_location)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use select by attributes to delete all rows (points) containing duration values greater than 20. This won't remove all errors, but it is a good start.\n",
    "\n",
    "in_layer_or_view = \"processed_points\"\n",
    "selection_type = \"NEW_SELECTION\"\n",
    "where_clause = time_assumption\n",
    "\n",
    "Time_Selection = arcpy.management.SelectLayerByAttribute(in_layer_or_view, selection_type, where_clause)\n",
    "\n",
    "arcpy.management.DeleteFeatures(Time_Selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 3: The preceding section is helpful for automatic removal of GPS tracks within ArcGIS Pro. However, GPS data can be noisy and may require some manual effort to identify lingering errors. For example, in the Grand Canyon study, some people left the study site for some time (e.g., to shop, camp outside the park boundaries) and then returned. Their data is still valuable, however, it could be mixed up with other data that is from device error (e.g., devices that recorded points in illogical destinations due to noise or topography), user error (e.g., a participant who appeared to have a device on them as they did a helicopter tour of the park) or researcher error (e.g., points recorded on GPS devices retrieved from visitor centers). The code below is intended to facilitate manual examination of data points that express unique or illogical behavior to help determine if they are valuable or warrant removal because they are derived from some form of error. Keep in mind, your geodatabase should still have individual records for each participant saved as well, in case that helps with data exploration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Parse for analysis and create data labels for clarity indicating the labels for points, participant identification, and field for time\n",
    "\n",
    "processed_points_2 = os.path.join(target_gdb, \"processed_points\")\n",
    "\n",
    "df = pd.DataFrame.spatial.from_featureclass(processed_points_2)\n",
    "df.rename(columns={'OBJECTID':'FID','group_id': 'GID', 'd_start':'dtime'},inplace=True)\n",
    "\n",
    "##Convert time to proper time field format\n",
    "df['dtime'] = pd.to_datetime(df['dtime'])\n",
    "\n",
    "##Sort data by field and ID\n",
    "df.sort_values(by=['GID', 'dtime'], inplace=True)\n",
    "df\n",
    "\n",
    "##Dataframe copy\n",
    "\n",
    "seq_timedf = df.copy()\n",
    "seq_timedf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Use the shift command to record the subsequently recorded point for each point from each participant\n",
    "\n",
    "seq_timedf.sort_values(by=['GID', 'dtime'], inplace=True)\n",
    "seq_timedf[['GID_2', 'dtime_2']] = seq_timedf.groupby('GID')[['GID', 'dtime']].shift(-1)\n",
    "seq_timedf['GID_2'] = seq_timedf['GID_2'].astype(str).str.split('.0')\n",
    "seq_timedf['GID_2'] = seq_timedf['GID_2'].str.get(0)\n",
    "seq_timedf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Identify a length of time between points that is likely to have occurred because of error, manually readjusting based on the subsequent steps until you consistently stop identifying errors. In our case, we did this until we reach 16 hours because lengths of time under that were consecutively explainable behavior for more than five instances (mostly people apparently camping).\n",
    "\n",
    "seq_timedf2 = seq_timedf.copy()\n",
    "seq_timedf2['POI_timedelta'] = seq_timedf2['dtime_2'] - seq_timedf2['dtime']\n",
    "seq_time_filt = seq_timedf2['POI_timedelta'] > '0 days 16:00:00'\n",
    "\n",
    "##Createa field to serve as a filter to identify when this occurs for each recorded point\n",
    "\n",
    "seq_timedf2['POI_timedelta_filt'] = seq_time_filt\n",
    "seq_timedf2['POI_timedelta_filt'] = seq_timedf2['POI_timedelta_filt'].astype(int)\n",
    "seq_timedf2['POI_timedelta_filt'] = seq_timedf2.groupby('GID')['POI_timedelta_filt'].cumsum()\n",
    "seq_timedf2['GID_2'] = seq_timedf2['GID'].astype(str)+'.'+seq_timedf2['POI_timedelta_filt'].astype(str)\n",
    "seq_timedf2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Create dataframe containing filter for time delta for manual evaluation\n",
    "\n",
    "seq_time_filt2 = seq_time_filt==True\n",
    "df10 = seq_timedf2.copy()\n",
    "df10 = pd.DataFrame(df10.loc[seq_time_filt2])\n",
    "df10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determine errant points and tracks based off of time and place that need to be deleted. Make your determination based on your best judgement due to (i) length of time from preceding points and (ii) visual examination of the points in ArcGIS. You can do this by looking the point field for seemingly errant points in ArcGIs and confirming if they indicate a spatial pattern that may result from error. For example, in our case, if points occurred near cabin or GPS device drop-off location/far away from preceding points in time and space.\n",
    "\n",
    "To conduct the manual examination, follow the code blocks below for each participant ID that was identified above as being susceptible to error. Record the entire tracks and individual points that caused issues below. Depending on your comfort with Python, you can follow the code below to delete these tracks, or, if you prefer, you can delete them by hand in ArcGIS.\n",
    "\n",
    "The next three blocks of code are for recording records as you work through the data. See the next markdown block for more information on how to identify errors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Identify tracks with errors. This deeper dive may sometimes help you identify errors from other steps in your process. For example, technicians may save errant tracks or mislabel them. These tracks may entirely be the results of error entirely and warrant removal, whereas other times they simply were mislabeled. Below are mislabeled tracks for deletion with justification based on visual examination:\n",
    "\n",
    "##[Your group ID here] researcher error (or other reasons for deletion) and should be deleted\n",
    "\n",
    "##Below are acceptable mislabeled tracks:\n",
    "##[Your tracks that are flagged as errors but are acceptable]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Identify points with errors. Notice that you should record your points for clarity, but it is the points in the next section that are used in the code formally.\n",
    "\n",
    "##Points with issues because...\n",
    "GID_last_points = [\"Your points here\"]\n",
    "\n",
    "##Points with issues because...\n",
    "GID_two_last_points =[\"Your points here\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Identify flagged points WITHOUT errors. These will be used in the code as cases to retain from the dataset.\n",
    "\n",
    "##Questionable points:\n",
    "##[List the points and why they may have been flagged here but why they likely aren't error] ##Example: 179912 has a series of points three days after the rest that are drive from GPD dropbox up desert view (don't end at cabins)\n",
    "\n",
    "##Odd behaviors, but data seems worth retaining:\n",
    "##[List the points and why they may have been flagged here but why they likely aren't error] ##Example: 010812 has lapses and stays in a cabin for a long time but doesn't seem like researcher error\n",
    "\n",
    "odd_data = [\"Your points here\"] ##Example: [\"010812\", \"016712\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the code below to create a dataframe listing all points from participant identifications flagged with having potential error individually. Open ArcGIs and use the \"Search by Attribute Function\" to highlight all points from a participant identification. Visually examine all points to identify a pattern that may explain the apparent error. Determine if the pattern suggests behavior worth retaining (e.g., visitor left the destination but later returned) and record it above for record. If the pattern appears to reveal an error instead (e.g., one or two points appear at the end of the track in a distant location, the device appears to have been dropped off at the dropbox and recorded the researcher driving through the destination), then record it appropriately above.Once you feel you are no longer consistently finding errors, continue with the code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Method for searching for errors in GID.\n",
    "\n",
    "error_GID = \"Your GID identified as having error\"\n",
    "seq_timedf10 = df.copy()\n",
    "seq_timedf_10 = seq_timedf10['GID'] == error_GID\n",
    "seq_timedf_10 = pd.DataFrame(seq_timedf10.loc[seq_timedf_10])\n",
    "seq_timedf_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next blocks of code will delete the points that you identify as having errors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Create dataframe of all values that come after large time delta with odd data excluded (i.e., data that was not from error, but odd user behavior), which will then inherently only retain data with errors.\n",
    "\n",
    "df11 = seq_timedf2.copy()\n",
    "seq_time_filt3 = df11[\"POI_timedelta_filt\"] == 1\n",
    "df11 = df11.loc[seq_time_filt3]\n",
    "df11 = df11.groupby('GID',group_keys=False).apply(lambda x:x[1:])\n",
    "df11 = df11[~df11[\"GID\"].isin(odd_data)]\n",
    "df11[\"GID\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Create a dataframe of points that should be deleted.\n",
    "\n",
    "Delete_Points = pd.DataFrame(df11[['FID','SHAPE']])\n",
    "Delete_Points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Create a dataframe of GIDs that need to be deleted (i.e., full participant tracks)\n",
    "\n",
    "Bad_GID = df.copy()\n",
    "Bad_GIDs = [\"The entire tracks that warrant removal based on your examination\"] ##Example [\"Current Track: 22 JUN 2\",\"Current Track: 05 JUN 2\"].\n",
    "Bad_GID2 = pd.DataFrame(Bad_GID[Bad_GID[\"GID\"].isin(Bad_GIDs)])\n",
    "Bad_GID2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Simplify the dataframe\n",
    "\n",
    "Bad_GID3 = pd.DataFrame(Bad_GID2[['FID','SHAPE']])\n",
    "Bad_GID3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Concatentate dataframes with points/tracks that must be deleted from points layer\n",
    "\n",
    "Delete_Points2 = pd.concat([Delete_Points, Bad_GID3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Import of dataframe to ArcGIS geodatabase\n",
    "\n",
    "Delete_Points2.spatial.to_featureclass(location=os.path.join(target_gdb, \"delete_points\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Select the overlapping points between the original shapefile and the shapefile with the points to delete\n",
    "\n",
    "in_layer = \"processed_points\"\n",
    "overlap_type = \"INTERSECT\"\n",
    "select_features = \"delete_points\"\n",
    "\n",
    "Error_Selection = arcpy.management.SelectLayerByLocation(in_layer, overlap_type, select_features,selection_type = \"NEW_SELECTION\")\n",
    "\n",
    "##Delete the points\n",
    "\n",
    "arcpy.management.DeleteFeatures(Error_Selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are still a few more dimensions to analyze for errors, specifically labeling and trip length."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##With the data downloaded, do some final house cleaning by examining descriptive attributes\n",
    "\n",
    "processed_points_3 = pd.DataFrame.spatial.from_featureclass(os.path.join(target_gdb, \"processed_points\"))\n",
    "processed_points_3.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Update fields as needed to proper labeling scheme\n",
    "\n",
    "processed_points_3.loc[processed_points_3[\"group_id\"] == [\"Current improper label\"], \"group_id\"] = \"Correct label\"\n",
    "##Example: processed_points_3.loc[processed_points_3[\"group_id\"] == \"Current Track: 10 JUN 2\", \"group_id\"] = \"152212\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Check trip length for consistency (both minimumum and maximum.\n",
    "\n",
    "trip_length = processed_points_3.groupby('group_id')['d_start'].agg(['min', 'max'])\n",
    "\n",
    "# Compute the difference in days between max and min 'd_start'\n",
    "trip_length['trip_length'] = (trip_length['max'] - trip_length['min'])\n",
    "\n",
    "trip_length.sort_values(by='trip_length')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete illogically long or short trips after examining them in ArcGIS Pro\n",
    "processed_points_3 = processed_points_3[~processed_points_3['group_id'].isin([\"Group ID's for tracks that are too short or long\"])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Check sample size and number of total observations\n",
    "\n",
    "uniqGID = processed_points_3['group_id'].nunique()\n",
    "print(\"unique GIDs:\" + str(uniqGID))\n",
    "waypoints = processed_points_3.shape[0]\n",
    "print(\"# of waypoints:\" + str(waypoints))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_points_3.spatial.to_featureclass(location=os.path.join(target_gdb, \"cleaned_points\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Delete obvious erroneous points manually. Recpord what you delete below (delete key in ArcGIS):\n",
    "##Example: 61 points around GPS device dropbox location outside the buffer range were manually deleted\n",
    "\n",
    "\n",
    "##Record the points that may also warrant removal but that you opted to retain (e.g., they have error, but it's minor or inconsequential for analysis)\n",
    "\n",
    "##Example: 11 points appear in a cluster in the canyon far off trail with no apparent lead up on Bright Angel Trailhead near Powell Point. These may be deleted, but again, that may be better to leave to the individual researcher based on their needs/analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
